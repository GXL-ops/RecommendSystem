{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uploading the Boston dataset\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Transforming the problem into a classification (unbalanced)\n",
    "y_bin = (y > np.percentile(y, 90)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#CRIM - per capita crime rate by town\n",
    "#ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "#INDUS - proportion of non-retail business acres per town.\n",
    "#CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "#NOX - nitric oxides concentration (parts per 10 million)\n",
    "#RM - average number of rooms per dwelling\n",
    "#AGE - proportion of owner-occupied units built prior to 1940\n",
    "#DIS - weighted distances to five Boston employment centres\n",
    "#RAD - index of accessibility to radial highways\n",
    "#TAX - full-value property-tax rate per $10,000\n",
    "#PTRATIO - pupil-teacher ratio by town\n",
    "#B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "#LSTAT - % lower status of the population\n",
    "#MEDV - Median value of owner-occupied homes in $1000's this is our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram highlighting the top 10% we use as a target\n",
    "plt.hist(y[y <= np.percentile(y, 90)], bins='auto', alpha=0.7, label='0', color='b')\n",
    "plt.hist(y[y > np.percentile(y, 90)], bins=8, alpha=0.7, label='1', color='r')\n",
    "plt.title(\"Histogram of MEDV\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, we will create a Pandas dataframe from X\n",
    "train = pd.DataFrame(X)\n",
    "train = train.add_prefix('var_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "# Checking about the shape of our training set\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a 5-fold stratified cross-validation (note: shuffle=True)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedKFold(n_splits=5, random_state=8, shuffle=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 2000\n",
    "lgb_iter1 = []\n",
    "sklearn_gbm_iter1 = []\n",
    "xgb_gbm_iter1 = []\n",
    "\n",
    "lgb_ap1 = []\n",
    "sklearn_gbm_ap1 = []\n",
    "xgb_gbm_ap1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the classifier with standard configuration\n",
    "# Later we will more performing parameters with Bayesian Optimization\n",
    "params = {\n",
    "    'learning_rate':  0.06, \n",
    "    'max_depth': 6, \n",
    "    #'lambda_l1': 16.7,\n",
    "    'min_data_in_leaf':5,\n",
    "    'boosting': 'gbdt', \n",
    "    'objective': 'binary', \n",
    "    'metric': 'auc',\n",
    "    'feature_fraction': .9,\n",
    "    'is_training_metric': False, \n",
    "    'seed': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      " Best iteration lgb =  48\n",
      " Best iteration sklearn_gbm =  137\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.944077134986226\n",
      "sklearn_gbn  0.6591545709192768\n",
      "xgboost  0.6591545709192768\n",
      "\n",
      "Fold  1\n",
      " Best iteration lgb =  115\n",
      " Best iteration sklearn_gbm =  199\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.9021031746031746\n",
      "sklearn_gbn  0.5538896746817539\n",
      "xgboost  0.5538896746817539\n",
      "\n",
      "Fold  2\n",
      " Best iteration lgb =  130\n",
      " Best iteration sklearn_gbm =  198\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.8108225108225108\n",
      "sklearn_gbn  0.5741474147414741\n",
      "xgboost  0.6016201620162017\n",
      "\n",
      "Fold  3\n",
      " Best iteration lgb =  32\n",
      " Best iteration sklearn_gbm =  66\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.9142857142857143\n",
      "sklearn_gbn  0.7297029702970297\n",
      "xgboost  0.7297029702970297\n",
      "\n",
      "Fold  4\n",
      " Best iteration lgb =  103\n",
      " Best iteration sklearn_gbm =  63\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.9909090909090909\n",
      "sklearn_gbn  0.5495049504950495\n",
      "xgboost  0.730913091309131\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(skf.split(train,y_bin)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y_bin[train_index], y_bin[test_index]\n",
    "    X_train, X_valid = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "        \n",
    "    print( \"\\nFold \", i)\n",
    "\n",
    "    # Running models for this fold\n",
    "    \n",
    "    # ->LightGBM\n",
    "    lgb_gbm = lgb.train(params, \n",
    "                          lgb.Dataset(X_train, label=y_train), \n",
    "                          MAX_ROUNDS, \n",
    "                          lgb.Dataset(X_valid, label=y_valid), \n",
    "                          verbose_eval=False, \n",
    "                          #feval= auc, \n",
    "                          early_stopping_rounds=50)\n",
    "    \n",
    "    print( \" Best iteration lgb = \", lgb_gbm.best_iteration)\n",
    "    \n",
    "    # ->Scikit-learn GBM\n",
    "    sklearn_gbm = GradientBoostingClassifier(n_estimators=MAX_ROUNDS, \n",
    "                                    learning_rate = 0.06,\n",
    "                                    max_features=2, \n",
    "                                    max_depth = 6, \n",
    "                                    n_iter_no_change=50, \n",
    "                                    tol=0.01,\n",
    "                                    random_state = 0)\n",
    "    \n",
    "    sklearn_gbm.fit(X_train, y_train)\n",
    "    print( \" Best iteration sklearn_gbm = \", sklearn_gbm.n_estimators_)\n",
    "    \n",
    "    # ->XGBoost\n",
    "    xgb_gbm = xgb.XGBClassifier(max_depth=6, \n",
    "                                n_estimators=MAX_ROUNDS,\n",
    "                                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                                learning_rate=0.06,\n",
    "                                early_stopping_rounds=50)\n",
    "    \n",
    "    xgb_gbm.fit(X_train, y_train)\n",
    "    \n",
    "    print( \" Best iteration xgboost_gbm = \", xgb_gbm.get_booster().best_iteration)\n",
    "    \n",
    "    # Storing and reporting results of the fold\n",
    "    lgb_iter1 = np.append(lgb_iter1, lgb_gbm.best_iteration)\n",
    "    sklearn_gbm_iter1 = np.append(sklearn_gbm_iter1, sklearn_gbm.n_estimators_)\n",
    "    xgb_gbm_iter1 = np.append(xgb_gbm_iter1, xgb_gbm.get_booster().best_iteration)\n",
    "   \n",
    "    pred = lgb_gbm.predict(X_valid, num_iteration=lgb_gbm.best_iteration)\n",
    "    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n",
    "    print('lgb ', ap)\n",
    "    lgb_ap1 = np.append(lgb_ap1, ap)\n",
    "    \n",
    "    pred = sklearn_gbm.predict(X_valid)\n",
    "    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n",
    "    print('sklearn_gbn ', ap)\n",
    "    sklearn_gbm_ap1 = np.append(sklearn_gbm_ap1, ap)\n",
    "    \n",
    "    pred  = xgb_gbm.predict(X_valid)\n",
    "    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n",
    "    print('xgboost ', ap)\n",
    "    xgb_gbm_ap1 = np.append(xgb_gbm_ap1, ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb_iter1:  85.6\n",
      "sklearn_gbm_iter1:  132.6\n",
      "xgb_gbm_iter1:  1999.0\n",
      "lgb_ap1:  0.9124395251213434\n",
      "sklearn_gbm_ap1:  0.6132799162269167\n",
      "xgb_gbm_ap1:  0.6550560938446786\n"
     ]
    }
   ],
   "source": [
    "print('lgb_iter1: ', np.mean(lgb_iter1))\n",
    "print('sklearn_gbm_iter1: ', np.mean(sklearn_gbm_iter1))\n",
    "print('xgb_gbm_iter1: ',np.mean(xgb_gbm_iter1))\n",
    "\n",
    "print('lgb_ap1: ', np.mean(lgb_ap1))\n",
    "print('sklearn_gbm_ap1: ', np.mean(sklearn_gbm_ap1))\n",
    "print('xgb_gbm_ap1: ', np.mean(xgb_gbm_ap1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>87616.0</td>\n",
       "      <td>4528.8</td>\n",
       "      <td>117482.40</td>\n",
       "      <td>1474.08</td>\n",
       "      <td>234.09</td>\n",
       "      <td>6072.570</td>\n",
       "      <td>76.194</td>\n",
       "      <td>157529.6100</td>\n",
       "      <td>1976.5620</td>\n",
       "      <td>24.8004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58564.0</td>\n",
       "      <td>4307.6</td>\n",
       "      <td>96049.80</td>\n",
       "      <td>2211.88</td>\n",
       "      <td>316.84</td>\n",
       "      <td>7064.820</td>\n",
       "      <td>162.692</td>\n",
       "      <td>157529.6100</td>\n",
       "      <td>3627.6660</td>\n",
       "      <td>83.5396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58564.0</td>\n",
       "      <td>4307.6</td>\n",
       "      <td>95064.86</td>\n",
       "      <td>975.26</td>\n",
       "      <td>316.84</td>\n",
       "      <td>6992.374</td>\n",
       "      <td>71.734</td>\n",
       "      <td>154315.4089</td>\n",
       "      <td>1583.1049</td>\n",
       "      <td>16.2409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49284.0</td>\n",
       "      <td>4151.4</td>\n",
       "      <td>87607.86</td>\n",
       "      <td>652.68</td>\n",
       "      <td>349.69</td>\n",
       "      <td>7379.581</td>\n",
       "      <td>54.978</td>\n",
       "      <td>155732.8369</td>\n",
       "      <td>1160.2122</td>\n",
       "      <td>8.6436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49284.0</td>\n",
       "      <td>4151.4</td>\n",
       "      <td>88111.80</td>\n",
       "      <td>1183.26</td>\n",
       "      <td>349.69</td>\n",
       "      <td>7422.030</td>\n",
       "      <td>99.671</td>\n",
       "      <td>157529.6100</td>\n",
       "      <td>2115.4770</td>\n",
       "      <td>28.4089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0        1     2     3    4      5      6     7       8    9    ...  \\\n",
       "0  1.0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  ...   \n",
       "1  1.0  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  ...   \n",
       "2  1.0  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  ...   \n",
       "3  1.0  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  ...   \n",
       "4  1.0  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  ...   \n",
       "\n",
       "       95      96         97       98      99        100      101  \\\n",
       "0  87616.0  4528.8  117482.40  1474.08  234.09  6072.570   76.194   \n",
       "1  58564.0  4307.6   96049.80  2211.88  316.84  7064.820  162.692   \n",
       "2  58564.0  4307.6   95064.86   975.26  316.84  6992.374   71.734   \n",
       "3  49284.0  4151.4   87607.86   652.68  349.69  7379.581   54.978   \n",
       "4  49284.0  4151.4   88111.80  1183.26  349.69  7422.030   99.671   \n",
       "\n",
       "           102        103      104  \n",
       "0  157529.6100  1976.5620  24.8004  \n",
       "1  157529.6100  3627.6660  83.5396  \n",
       "2  154315.4089  1583.1049  16.2409  \n",
       "3  155732.8369  1160.2122   8.6436  \n",
       "4  157529.6100  2115.4770  28.4089  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "poly_train = poly.fit_transform(train)\n",
    "poly_train = pd.DataFrame(poly_train)\n",
    "poly_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>poly_95</th>\n",
       "      <th>poly_96</th>\n",
       "      <th>poly_97</th>\n",
       "      <th>poly_98</th>\n",
       "      <th>poly_99</th>\n",
       "      <th>poly_100</th>\n",
       "      <th>poly_101</th>\n",
       "      <th>poly_102</th>\n",
       "      <th>poly_103</th>\n",
       "      <th>poly_104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>...</td>\n",
       "      <td>87616.0</td>\n",
       "      <td>4528.8</td>\n",
       "      <td>117482.40</td>\n",
       "      <td>1474.08</td>\n",
       "      <td>234.09</td>\n",
       "      <td>6072.570</td>\n",
       "      <td>76.194</td>\n",
       "      <td>157529.6100</td>\n",
       "      <td>1976.5620</td>\n",
       "      <td>24.8004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58564.0</td>\n",
       "      <td>4307.6</td>\n",
       "      <td>96049.80</td>\n",
       "      <td>2211.88</td>\n",
       "      <td>316.84</td>\n",
       "      <td>7064.820</td>\n",
       "      <td>162.692</td>\n",
       "      <td>157529.6100</td>\n",
       "      <td>3627.6660</td>\n",
       "      <td>83.5396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58564.0</td>\n",
       "      <td>4307.6</td>\n",
       "      <td>95064.86</td>\n",
       "      <td>975.26</td>\n",
       "      <td>316.84</td>\n",
       "      <td>6992.374</td>\n",
       "      <td>71.734</td>\n",
       "      <td>154315.4089</td>\n",
       "      <td>1583.1049</td>\n",
       "      <td>16.2409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49284.0</td>\n",
       "      <td>4151.4</td>\n",
       "      <td>87607.86</td>\n",
       "      <td>652.68</td>\n",
       "      <td>349.69</td>\n",
       "      <td>7379.581</td>\n",
       "      <td>54.978</td>\n",
       "      <td>155732.8369</td>\n",
       "      <td>1160.2122</td>\n",
       "      <td>8.6436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49284.0</td>\n",
       "      <td>4151.4</td>\n",
       "      <td>88111.80</td>\n",
       "      <td>1183.26</td>\n",
       "      <td>349.69</td>\n",
       "      <td>7422.030</td>\n",
       "      <td>99.671</td>\n",
       "      <td>157529.6100</td>\n",
       "      <td>2115.4770</td>\n",
       "      <td>28.4089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var_0  var_1  var_2  var_3  var_4  var_5  var_6   var_7  var_8  var_9  \\\n",
       "0  0.00632   18.0   2.31    0.0  0.538  6.575   65.2  4.0900    1.0  296.0   \n",
       "1  0.02731    0.0   7.07    0.0  0.469  6.421   78.9  4.9671    2.0  242.0   \n",
       "2  0.02729    0.0   7.07    0.0  0.469  7.185   61.1  4.9671    2.0  242.0   \n",
       "3  0.03237    0.0   2.18    0.0  0.458  6.998   45.8  6.0622    3.0  222.0   \n",
       "4  0.06905    0.0   2.18    0.0  0.458  7.147   54.2  6.0622    3.0  222.0   \n",
       "\n",
       "   ...  poly_95  poly_96    poly_97  poly_98  poly_99  poly_100  poly_101  \\\n",
       "0  ...  87616.0   4528.8  117482.40  1474.08   234.09  6072.570    76.194   \n",
       "1  ...  58564.0   4307.6   96049.80  2211.88   316.84  7064.820   162.692   \n",
       "2  ...  58564.0   4307.6   95064.86   975.26   316.84  6992.374    71.734   \n",
       "3  ...  49284.0   4151.4   87607.86   652.68   349.69  7379.581    54.978   \n",
       "4  ...  49284.0   4151.4   88111.80  1183.26   349.69  7422.030    99.671   \n",
       "\n",
       "      poly_102   poly_103  poly_104  \n",
       "0  157529.6100  1976.5620   24.8004  \n",
       "1  157529.6100  3627.6660   83.5396  \n",
       "2  154315.4089  1583.1049   16.2409  \n",
       "3  155732.8369  1160.2122    8.6436  \n",
       "4  157529.6100  2115.4770   28.4089  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_train = poly_train.add_prefix('poly_')\n",
    "train = pd.concat([train,poly_train], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 2000\n",
    "lgb_iter2 = []\n",
    "sklearn_gbm_iter2 = []\n",
    "xgb_gbm_iter2 = []\n",
    "\n",
    "lgb_ap2 = []\n",
    "sklearn_gbm_ap2 = []\n",
    "xgb_gbm_ap2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      " Best iteration lgb =  30\n",
      " Best iteration sklearn_gbm =  146\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.9715909090909091\n",
      "sklearn_gbn  0.8362502025603629\n",
      "xgboost  0.6158645276292334\n",
      "\n",
      "Fold  1\n",
      " Best iteration lgb =  92\n",
      " Best iteration sklearn_gbm =  145\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.8987698412698412\n",
      "sklearn_gbn  0.4661716171617162\n",
      "xgboost  0.5538896746817539\n",
      "\n",
      "Fold  2\n",
      " Best iteration lgb =  37\n",
      " Best iteration sklearn_gbm =  108\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.9206196581196582\n",
      "sklearn_gbn  0.4661716171617162\n",
      "xgboost  0.6598019801980199\n",
      "\n",
      "Fold  3\n",
      " Best iteration lgb =  49\n",
      " Best iteration sklearn_gbm =  62\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.8938339438339438\n",
      "sklearn_gbn  0.40664780763790664\n",
      "xgboost  0.8198019801980199\n",
      "\n",
      "Fold  4\n",
      " Best iteration lgb =  1\n",
      " Best iteration sklearn_gbm =  79\n",
      " Best iteration xgboost_gbm =  1999\n",
      "lgb  0.8742063492063492\n",
      "sklearn_gbn  0.4594059405940594\n",
      "xgboost  0.7297029702970297\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(skf.split(train,y_bin)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y_bin[train_index], y_bin[test_index]\n",
    "    X_train, X_valid = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "        \n",
    "    print( \"\\nFold \", i)\n",
    "\n",
    "    # Run model for this fold\n",
    "\n",
    "    lgb_gbm = lgb.train(params, \n",
    "                          lgb.Dataset(X_train, label=y_train), \n",
    "                          MAX_ROUNDS, \n",
    "                          lgb.Dataset(X_valid, label=y_valid), \n",
    "                          verbose_eval=False, \n",
    "                          #feval= auc, \n",
    "                          early_stopping_rounds=50)\n",
    "    \n",
    "    print( \" Best iteration lgb = \", lgb_gbm.best_iteration)\n",
    "    \n",
    "    sklearn_gbm = GradientBoostingClassifier(n_estimators=MAX_ROUNDS, \n",
    "                                    learning_rate = 0.06,\n",
    "                                    max_features=2, \n",
    "                                    max_depth = 6, \n",
    "                                    n_iter_no_change=50, \n",
    "                                    tol=0.01,\n",
    "                                    random_state = 0)\n",
    "    \n",
    "    sklearn_gbm.fit(X_train, y_train)\n",
    "    print( \" Best iteration sklearn_gbm = \", sklearn_gbm.n_estimators_)\n",
    "    \n",
    "    \n",
    "    xgb_gbm = xgb.XGBClassifier(max_depth=6, \n",
    "                                n_estimators=MAX_ROUNDS,\n",
    "                                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                                learning_rate=0.06,\n",
    "                                early_stopping_rounds=50)\n",
    "\n",
    "    xgb_gbm.fit(X_train, y_train)\n",
    "    \n",
    "    print( \" Best iteration xgboost_gbm = \", xgb_gbm.get_booster().best_iteration)\n",
    "    \n",
    "    lgb_iter2 = np.append(lgb_iter2, lgb_gbm.best_iteration)\n",
    "    sklearn_gbm_iter2 = np.append(sklearn_gbm_iter2, sklearn_gbm.n_estimators_)\n",
    "    xgb_gbm_iter2 = np.append(xgb_gbm_iter2, xgb_gbm.get_booster().best_iteration)\n",
    "    \n",
    "    pred = lgb_gbm.predict(X_valid, num_iteration=lgb_gbm.best_iteration)\n",
    "    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n",
    "    print('lgb ', ap)\n",
    "    lgb_ap2 = np.append(lgb_ap2, ap)\n",
    "    \n",
    "    pred = sklearn_gbm.predict(X_valid)\n",
    "    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n",
    "    print('sklearn_gbn ', ap)\n",
    "    sklearn_gbm_ap2 = np.append(sklearn_gbm_ap2, ap)\n",
    "    \n",
    "    pred  = xgb_gbm.predict(X_valid)\n",
    "    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n",
    "    print('xgboost ', ap)\n",
    "    xgb_gbm_ap2 = np.append(xgb_gbm_ap2, ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb_iter1:  85.6  lgb_iter2:  41.8\n",
      "sklearn_gbm_iter1:  132.6  sklearn_gbm_iter2:  108.0\n",
      "xgb_gbm_iter1:  1999.0  xgb_gbm_iter2:  1999.0\n",
      "lgb_ap1:  0.9124395251213434  lgb_ap2:  0.9118041403041403\n",
      "sklearn_gbm_ap1:  0.6132799162269167  sklearn_gbm_ap2:  0.5269294370231522\n",
      "xgb_gbm_ap1:  0.6550560938446786  xgb_gbm_ap2:  0.6758122266008113\n"
     ]
    }
   ],
   "source": [
    "print('lgb_iter1: ', np.mean(lgb_iter1),' lgb_iter2: ', np.mean(lgb_iter2))\n",
    "print('sklearn_gbm_iter1: ', np.mean(sklearn_gbm_iter1), ' sklearn_gbm_iter2: ', np.mean(sklearn_gbm_iter2))\n",
    "print('xgb_gbm_iter1: ',np.mean(xgb_gbm_iter1), ' xgb_gbm_iter2: ',np.mean(xgb_gbm_iter2) )\n",
    "\n",
    "print('lgb_ap1: ', np.mean(lgb_ap1), ' lgb_ap2: ', np.mean(lgb_ap2))\n",
    "print('sklearn_gbm_ap1: ', np.mean(sklearn_gbm_ap1), ' sklearn_gbm_ap2: ', np.mean(sklearn_gbm_ap2))\n",
    "print('xgb_gbm_ap1: ', np.mean(xgb_gbm_ap1), ' xgb_gbm_ap2: ', np.mean(xgb_gbm_ap2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e2c0931287d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Skopt functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mskopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBayesSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgp_minimize\u001b[0m \u001b[1;31m# Bayesian optimization using Gaussian Processes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspace\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInteger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skopt'"
     ]
    }
   ],
   "source": [
    "# Importing core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "\n",
    "# Suppressing warnings because of skopt verbosity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Our example dataset\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Hyperparameters distributions\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Skopt functions\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import gp_minimize # Bayesian optimization using Gaussian Processes\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\n",
    "from skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\n",
    "from skopt.callbacks import VerboseCallback # Callback to control the verbosity\n",
    "from skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
